import numpy as np
from numpy import newaxis
import math
import pandas as pd
import sklearn
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold, cross_validate,cross_val_predict,cross_val_score
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

class Decision_Tree():
    def __init__(self, X = None, y = None, X_train = None, y_train = None, X_test = None, y_test = None):
        self.X = X
        self.y = y
        self.X_train =  X_train
        self.y_train = y_train
        self.X_train = X_train
        self.y_train = y_train

    def set_X_y(self, filepath):
        self.X, self.y =

        # based on the last plot generated, can choose optimal ccp_alpha value for the model + criterion
        # (i.e. the model that minimizes the test error)
        # then use this model to do bootstrapping to comment on variance & confidence interval
        # and k-fold vs OOB accuracy to comment on the accuracy
    def model_and_plot(self, criterion="gini"):
        # adopted from https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, random_state=0)

        clf = DecisionTreeClassifier(random_state=0, criterion=criterion)
        path = clf.cost_complexity_pruning_path(self.X_train, self.y_train)
        ccp_alphas, impurities = path.ccp_alphas, path.impurities

        fig, ax = plt.subplots()
        ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle="steps-post")
        ax.set_xlabel("effective alpha")
        ax.set_ylabel("total impurity of leaves")
        ax.set_title("Total Impurity vs effective alpha for training set")

        clfs = []
        for ccp_alpha in ccp_alphas:
            clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha, criterion=criterion)
            clf.fit(self.X_train, self.y_train)
            clfs.append(clf)
        print("Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
            clfs[-1].tree_.node_count, ccp_alphas[-1]))

        clfs = clfs[:-1]
        ccp_alphas = ccp_alphas[:-1]

        node_counts = [clf.tree_.node_count for clf in clfs]
        depth = [clf.tree_.max_depth for clf in clfs]
        fig, ax = plt.subplots(2, 1)
        ax[0].plot(ccp_alphas, node_counts, marker='o', drawstyle="steps-post")
        ax[0].set_xlabel("alpha")
        ax[0].set_ylabel("number of nodes")
        ax[0].set_title("Number of nodes vs alpha")
        ax[1].plot(ccp_alphas, depth, marker='o', drawstyle="steps-post")
        ax[1].set_xlabel("alpha")
        ax[1].set_ylabel("depth of tree")
        ax[1].set_title("Depth vs alpha")
        fig.tight_layout()

        train_scores = [clf.score(self.X_train, self.y_train) for clf in clfs]
        test_scores = [clf.score(self.X_test, self.y_test) for clf in clfs]

        fig, ax = plt.subplots()
        ax.set_xlabel("alpha")
        ax.set_ylabel("accuracy")
        ax.set_title("Accuracy vs alpha for training and testing sets")
        ax.plot(ccp_alphas, train_scores, marker='o', label="train",
                drawstyle="steps-post")
        ax.plot(ccp_alphas, test_scores, marker='o', label="test",
                drawstyle="steps-post")
        ax.legend()
        plt.show()

